{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Basic Anomaly Detection for Cybersecurity Logs (Synthetic Dataset)\n\nThis notebook builds a small end-to-end anomaly detection pipeline for cybersecurity-style logs, including:\n\n- **Dataset generation** (synthetic login + network-like fields)\n- **EDA**\n- **Isolation Forest** anomaly detection (with proper preprocessing)\n- **2D projection (PCA)** and anomaly visualization\n- **Conclusion**\n\n**MITRE ATT&CK technique covered:**  \n- **T1110 – Brute Force** (many failed login attempts and unusual patterns)\n- *(optional interpretation)* **T1078 – Valid Accounts** (successful but suspicious logins at unusual hours/locations)\n\n---\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n\n# Reproducibility\nRANDOM_STATE = 42\nrng = np.random.default_rng(RANDOM_STATE)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Prepare a cybersecurity-related dataset (Synthetic)\n\nWe generate a synthetic **login event** dataset with these columns:\n\n- Time-based feature: `hour`\n- Numeric features: `duration_sec`, `bytes_sent`, `failed_attempts_last_10m`\n- Categorical features: `user`, `src_country`, `protocol`\n- Optional evaluation label: `label` (0=normal, 1=attack)\n\n**Anomalies (~2%)** represent **Brute Force (T1110)** patterns, such as:\n- Elevated `failed_attempts_last_10m`\n- Off-hours activity (late night / early morning)\n- Unusual geography or protocol mix\n- Slightly longer durations and/or abnormal bytes behavior\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef generate_synthetic_cyber_logs(n_rows=12000, anomaly_ratio=0.02, random_state=42):\n    rng = np.random.default_rng(random_state)\n    \n    n_anom = int(n_rows * anomaly_ratio)\n    n_norm = n_rows - n_anom\n    \n    # Categorical pools\n    users = [f\"user_{i:02d}\" for i in range(1, 21)]  # 20 users\n    countries_common = [\"IL\", \"US\", \"DE\", \"FR\", \"UK\", \"NL\"]\n    countries_rare = [\"RU\", \"CN\", \"IR\", \"KP\"]  # 'rare' geos for anomalies\n    protocols = [\"SSH\", \"RDP\", \"HTTPS\"]\n    \n    # -------- Normal behavior --------\n    # Hours: mostly work hours + evening\n    hour_norm = np.clip(np.round(rng.normal(loc=14, scale=4, size=n_norm)).astype(int), 0, 23)\n    \n    # Duration (seconds): short typical sessions\n    duration_norm = np.clip(rng.lognormal(mean=2.0, sigma=0.35, size=n_norm), 2, 300)  # ~7s to ~300s\n    \n    # Bytes sent: modest with occasional medium; log-normal\n    bytes_norm = np.clip(rng.lognormal(mean=9.0, sigma=0.6, size=n_norm), 200, 300000).astype(int)\n    \n    # Failed attempts in last 10 minutes: mostly 0-1\n    failed_norm = rng.poisson(lam=0.25, size=n_norm)\n    \n    user_norm = rng.choice(users, size=n_norm, replace=True, p=None)\n    country_norm = rng.choice(countries_common, size=n_norm, replace=True, p=[0.45, 0.15, 0.12, 0.10, 0.10, 0.08])\n    protocol_norm = rng.choice(protocols, size=n_norm, replace=True, p=[0.55, 0.20, 0.25])\n    \n    df_norm = pd.DataFrame({\n        \"hour\": hour_norm,\n        \"duration_sec\": duration_norm,\n        \"bytes_sent\": bytes_norm,\n        \"failed_attempts_last_10m\": failed_norm,\n        \"user\": user_norm,\n        \"src_country\": country_norm,\n        \"protocol\": protocol_norm,\n        \"label\": np.zeros(n_norm, dtype=int)\n    })\n    \n    # -------- Anomalous behavior (T1110 Brute Force) --------\n    # Hours: off-hours concentrated (0-5, 22-23)\n    off_hours = np.array([0,1,2,3,4,5,22,23])\n    hour_anom = rng.choice(off_hours, size=n_anom, replace=True, p=[0.12,0.14,0.14,0.14,0.12,0.10,0.12,0.12])\n    \n    # Brute force: high failed attempts\n    failed_anom = np.clip(rng.poisson(lam=12, size=n_anom) + rng.integers(0, 8, size=n_anom), 6, 40)\n    \n    # Duration: sometimes longer (retries / automation), but still bounded\n    duration_anom = np.clip(rng.lognormal(mean=2.4, sigma=0.5, size=n_anom), 3, 600)\n    \n    # Bytes: can be low (failed auth) OR weird spikes (tooling/noise)\n    mix = rng.random(n_anom)\n    bytes_anom = np.where(\n        mix < 0.75,\n        rng.integers(50, 900, size=n_anom),                     # mostly tiny bytes during failed attempts\n        rng.integers(250000, 1200000, size=n_anom)              # occasional spikes\n    ).astype(int)\n    \n    # Target fewer users to mimic account targeting\n    targeted_users = rng.choice(users, size=5, replace=False)\n    user_anom = rng.choice(targeted_users, size=n_anom, replace=True)\n    \n    country_anom = rng.choice(countries_rare + countries_common, size=n_anom, replace=True,\n                              p=[0.22, 0.22, 0.20, 0.06, 0.12, 0.08, 0.05, 0.03, 0.02, 0.00])[:n_anom]\n    # (Above p vector length may be off if edited; ensure correct below)\n    \n    # Correct country sampling robustly:\n    all_countries = countries_rare + countries_common\n    p = np.array([0.22, 0.22, 0.20, 0.06, 0.12, 0.08, 0.05, 0.03, 0.02, 0.00], dtype=float)\n    p = p[:len(all_countries)]\n    p = p / p.sum()\n    country_anom = rng.choice(all_countries, size=n_anom, replace=True, p=p)\n    \n    # Protocol: more SSH/RDP in brute force attempts\n    protocol_anom = rng.choice(protocols, size=n_anom, replace=True, p=[0.70, 0.25, 0.05])\n    \n    df_anom = pd.DataFrame({\n        \"hour\": hour_anom,\n        \"duration_sec\": duration_anom,\n        \"bytes_sent\": bytes_anom,\n        \"failed_attempts_last_10m\": failed_anom,\n        \"user\": user_anom,\n        \"src_country\": country_anom,\n        \"protocol\": protocol_anom,\n        \"label\": np.ones(n_anom, dtype=int)\n    })\n    \n    # Combine and shuffle\n    df = pd.concat([df_norm, df_anom], ignore_index=True)\n    df = df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n    \n    # Round duration for readability\n    df[\"duration_sec\"] = df[\"duration_sec\"].round(2)\n    return df\n\ndf = generate_synthetic_cyber_logs(n_rows=12000, anomaly_ratio=0.02, random_state=RANDOM_STATE)\ndf.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Exploratory Data Analysis (EDA)\n\nWe compute basic stats and plot at least two visualizations:\n- Histograms for numeric features\n- Count plots for categorical features\n- Time-based distribution (logins per hour)\n\nThen we provide a short analytical summary.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Basic dataset statistics\nn_rows, n_cols = df.shape\nprint(\"Rows:\", n_rows)\nprint(\"Features:\", n_cols)\nprint(\"\\nClass distribution (label):\")\nprint(df[\"label\"].value_counts())\nprint(\"\\nClass distribution (%):\")\nprint((df[\"label\"].value_counts(normalize=True) * 100).round(2))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Numeric feature histograms\nnumeric_cols = [\"duration_sec\", \"bytes_sent\", \"failed_attempts_last_10m\", \"hour\"]\n\ndf[numeric_cols].hist(bins=40, figsize=(12, 8))\nplt.suptitle(\"Histograms of Numeric Features\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Count plots for categorical features (simple matplotlib-based counts)\ncat_cols = [\"user\", \"src_country\", \"protocol\"]\n\nfor col in cat_cols:\n    counts = df[col].value_counts()\n    plt.figure(figsize=(10, 4))\n    plt.bar(counts.index.astype(str), counts.values)\n    plt.title(f\"Count Plot: {col}\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.show()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Time-based distribution: events per hour (and compare normal vs attack)\nhour_counts_total = df[\"hour\"].value_counts().sort_index()\nhour_counts_norm = df[df[\"label\"] == 0][\"hour\"].value_counts().sort_index()\nhour_counts_anom = df[df[\"label\"] == 1][\"hour\"].value_counts().sort_index()\n\nhours = np.arange(24)\ntotal = hour_counts_total.reindex(hours, fill_value=0).values\nnorm = hour_counts_norm.reindex(hours, fill_value=0).values\nanom = hour_counts_anom.reindex(hours, fill_value=0).values\n\nplt.figure(figsize=(12, 4))\nplt.plot(hours, total, marker=\"o\")\nplt.title(\"Events per Hour (Total)\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Count\")\nplt.xticks(hours)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12, 4))\nplt.plot(hours, norm, marker=\"o\", label=\"Normal\")\nplt.plot(hours, anom, marker=\"o\", label=\"Attack\")\nplt.title(\"Events per Hour (Normal vs Attack)\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Count\")\nplt.xticks(hours)\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Analytical summary (3–5 sentences)\n\nIn this synthetic dataset, most normal logins occur around typical daytime and evening hours, with short-to-moderate session durations and modest byte volumes. Failed login attempts within a 10-minute window are usually near zero in normal behavior. We expect anomalies to appear as **off-hours activity** combined with **high failed-attempt counts**, consistent with **T1110 (Brute Force)**. Additionally, anomalies may show unusual country/protocol combinations and atypical byte patterns (very low bytes for failures, or occasional spikes). These patterns should be separable by an unsupervised detector like Isolation Forest.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Apply an Anomaly Detection Model (Isolation Forest)\n\nWe will:\n- Encode categorical features (OneHotEncoder)\n- Scale numeric features (StandardScaler)\n- Train Isolation Forest\n- Extract anomaly scores and predicted labels\n- Evaluate (scores histogram + anomaly count + metrics using ground truth label)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Features\nfeature_cols = [\"hour\", \"duration_sec\", \"bytes_sent\", \"failed_attempts_last_10m\", \"user\", \"src_country\", \"protocol\"]\ntarget_col = \"label\"\n\nX = df[feature_cols].copy()\ny = df[target_col].copy()  # for evaluation only\n\nnumeric_features = [\"hour\", \"duration_sec\", \"bytes_sent\", \"failed_attempts_last_10m\"]\ncategorical_features = [\"user\", \"src_country\", \"protocol\"]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ],\n    remainder=\"drop\"\n)\n\n# Isolation Forest: contamination ~ expected anomaly fraction\ncontamination = 0.02\n\niso = IsolationForest(\n    n_estimators=300,\n    contamination=contamination,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nmodel = Pipeline(steps=[\n    (\"preprocess\", preprocess),\n    (\"iso\", iso)\n])\n\nmodel\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Train Isolation Forest (unsupervised)\nmodel.fit(X)\n\n# Scores: decision_function -> higher means more normal; lower means more anomalous\nscores = model.named_steps[\"iso\"].decision_function(model.named_steps[\"preprocess\"].transform(X))\n\n# Predictions: IsolationForest returns 1 for inliers, -1 for outliers\npred_raw = model.named_steps[\"iso\"].predict(model.named_steps[\"preprocess\"].transform(X))\npred_anom = (pred_raw == -1).astype(int)  # 1=anomaly, 0=normal\n\ndf_results = df.copy()\ndf_results[\"anomaly_score\"] = scores\ndf_results[\"pred_anomaly\"] = pred_anom\n\ndf_results.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Histogram of anomaly scores\nplt.figure(figsize=(10, 4))\nplt.hist(df_results[\"anomaly_score\"], bins=50)\nplt.title(\"Histogram of Isolation Forest Anomaly Scores (higher=more normal)\")\nplt.xlabel(\"Anomaly score (decision_function)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n# How many anomalies detected\ndetected = df_results[\"pred_anomaly\"].sum()\nprint(\"Detected anomalies:\", detected, f\"({detected/len(df_results)*100:.2f}%)\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Evaluation with ground truth label (optional but recommended)\nacc = accuracy_score(y, pred_anom)\nprec = precision_score(y, pred_anom, zero_division=0)\nrec = recall_score(y, pred_anom, zero_division=0)\n\nprint(\"Accuracy:\", round(acc, 4))\nprint(\"Precision:\", round(prec, 4))\nprint(\"Recall:\", round(rec, 4))\nprint(\"\\nConfusion Matrix [ [TN, FP], [FN, TP] ]:\")\nprint(confusion_matrix(y, pred_anom))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y, pred_anom, digits=4))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Visualize anomalies on a 2D projection (PCA)\n\nWe apply PCA on the **preprocessed feature space** (scaled numerics + one-hot categories) and plot the first two principal components.\n\nWe color points by:\n- predicted anomaly label (normal vs anomaly)\n- (optionally) score intensity can be added, but label coloring is enough for the assignment.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Transform X into the model's feature space\nX_transformed = model.named_steps[\"preprocess\"].transform(X)\n\n# PCA to 2D\npca = PCA(n_components=2, random_state=RANDOM_STATE)\nX_2d = pca.fit_transform(X_transformed.toarray() if hasattr(X_transformed, \"toarray\") else X_transformed)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_, \" (sum=\", pca.explained_variance_ratio_.sum(), \")\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# 2D scatter plot colored by predicted anomalies\nplt.figure(figsize=(10, 6))\nmask_anom = df_results[\"pred_anomaly\"] == 1\n\nplt.scatter(X_2d[~mask_anom, 0], X_2d[~mask_anom, 1], s=8, alpha=0.6, label=\"Predicted Normal\")\nplt.scatter(X_2d[mask_anom, 0], X_2d[mask_anom, 1], s=18, alpha=0.8, label=\"Predicted Anomaly\")\n\nplt.title(\"PCA Projection (2D) with Isolation Forest Predictions\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Optional: Color by anomaly score (continuous)\nplt.figure(figsize=(10, 6))\nplt.scatter(X_2d[:, 0], X_2d[:, 1], s=10, alpha=0.7, c=df_results[\"anomaly_score\"])\nplt.title(\"PCA Projection (2D) Colored by Anomaly Score\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\ncbar = plt.colorbar()\ncbar.set_label(\"Anomaly score (higher=more normal)\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Interpretation (2–3 sentences)\n\nIn the PCA projection, normal behavior tends to form a **dense central cloud** because most events share similar time-of-day, low failed-attempt counts, and common user/country/protocol patterns. Predicted anomalies appear as **isolated points or small sparse groups**, often separated because they combine off-hours activity with unusually high failed attempts and rare categorical combinations. This is consistent with the expected structure for brute-force style anomalies.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conclusion\n\nThis pipeline successfully demonstrates an end-to-end anomaly detection workflow on cybersecurity-like logs. Isolation Forest identifies a small fraction of events as anomalous, primarily driven by **high failed login attempts**, **off-hours timing**, and **unusual categorical combinations** (users/countries/protocols), which align with **MITRE ATT&CK T1110 (Brute Force)**. The 2D PCA visualization shows normal events clustering densely, while anomalies appear more scattered at the edges, supporting the model’s ability to separate rare suspicious behavior from typical activity.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
